{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Library**"
      ],
      "metadata": {
        "id": "avz6EKeCayKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade git+https://github.com/ariaghora/mpstemmer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dDJ-J6AfkgP",
        "outputId": "7f623938-5b49-4e36-acec-38cae18e3083"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ariaghora/mpstemmer.git\n",
            "  Cloning https://github.com/ariaghora/mpstemmer.git to /tmp/pip-req-build-vutgjydu\n",
            "  Running command git clone -q https://github.com/ariaghora/mpstemmer.git /tmp/pip-req-build-vutgjydu\n",
            "Building wheels for collected packages: mpstemmer\n",
            "  Building wheel for mpstemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpstemmer: filename=mpstemmer-0.1.0-py3-none-any.whl size=99820 sha256=ff0a40a48720082e4e6de065ea498ad61e29f5d598586e9c193bf524ddee52e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v3ihgmoi/wheels/5c/f4/b7/9a03c2b80553c1ef45ee7971522137e4cd51db0ac5752f8d8a\n",
            "Successfully built mpstemmer\n",
            "Installing collected packages: mpstemmer\n",
            "Successfully installed mpstemmer-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "MRL5jS-6f3nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ry9Fy6baT-8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Dataset**"
      ],
      "metadata": {
        "id": "QKhcSYlqa5a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Es0qZKNrhvT_O3xPJC6riqdz8fQazfSg"
      ],
      "metadata": {
        "id": "8Ju_zkzfa5AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "df = pd.read_excel('/content/Kel5_Clickbait_Fix.xlsx')\n",
        "df = df[['Judul', 'Label_Akhir', 'Judul_Casefold', 'Judul_Relevant', 'Judul_Clean_Unlisted']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OqEhZDdQa8UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ==== PENTING ====\n",
        "df = df[df['Label_Akhir'] != 999]\n",
        "df = df.dropna()\n",
        "# df = df.sample(n=5000)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uUngYcvja-Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "00zijxt2fomO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "J_wAPLBtfq0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Tg3Zu8omfsaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def word_tokenize_wrapper(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "df['Judul_Tokenized'] = df['Judul_Relevant'].apply(word_tokenize_wrapper)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3D1Tfz7nf0cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "8SQpJRBgfsjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpstemmer import MPStemmer\n",
        "\n",
        "stemmer = MPStemmer()\n",
        "\n",
        "def stemming(words):\n",
        "  return [stemmer.stem(word) for word in words]\n",
        "\n",
        "df['Judul_Stemmed'] = df['Judul_Tokenized'].apply(stemming)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ajw6AeZkftXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords Removal"
      ],
      "metadata": {
        "id": "TpN3xo80ftfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "VawsrLvgfvzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_stopwords = stopwords.words('indonesian')\n",
        "list_stopwords = set(list_stopwords)\n",
        "print(list_stopwords)"
      ],
      "metadata": {
        "id": "6LKJCSJSf9pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(words):\n",
        "  return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['Judul_Clean'] = df['Judul_Stemmed'].apply(remove_stopwords)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KXAUWK5Mf_by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling**"
      ],
      "metadata": {
        "id": "drWCRKg5dt6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Corpus DIctionary"
      ],
      "metadata": {
        "id": "vNiYtJISdwgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary(df['Judul_Clean'])\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "q5p5ObE3bHWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "hZO0PTSygOQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in df['Judul_Clean']]\n",
        "\n",
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "from pprint import pprint\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "metadata": {
        "id": "5dGkkNXcdzJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coherence Values"
      ],
      "metadata": {
        "id": "Y3CyuinkgVBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from numpy import array\n",
        "#function to compute coherence values\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "    return model_list, coherence_values\n",
        "\n",
        "start=1\n",
        "limit=11\n",
        "step=1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
        "                                                        texts=df['Judul_Clean'], start=start, limit=limit, step=step)\n",
        "#show graphs\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "swprrAB6gP43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 6))"
      ],
      "metadata": {
        "id": "DBYp2-NWgZBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=7) #num topic menyesuaikan hasil dari coherence value paling tinggi\n",
        "for idx, topic in model.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic)) "
      ],
      "metadata": {
        "id": "12h808oMgtPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply LDA Model"
      ],
      "metadata": {
        "id": "9dyLkMFkheSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer()\n",
        "vect_text = vect.fit_transform(df['Judul_Clean_Unlisted'])"
      ],
      "metadata": {
        "id": "iz7CAskjhVmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model = LatentDirichletAllocation(n_components=7, \n",
        "                                    learning_method='online',\n",
        "                                    random_state=42,max_iter=1) \n",
        "lda_top = lda_model.fit_transform(vect_text)"
      ],
      "metadata": {
        "id": "uXtE22NShkie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Document 1: \")\n",
        "for i,topic in enumerate(lda_top[1]):\n",
        "  print(\"Topic \",i,\": \",topic*100,\"%\")"
      ],
      "metadata": {
        "id": "S6txvfzMiL-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grab the highest probability word per topics\n",
        "single_topic = lda_model.components_[0]"
      ],
      "metadata": {
        "id": "mlPUexA0iO4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_topic.argsort()"
      ],
      "metadata": {
        "id": "ycskqSzhiUNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vect.get_feature_names()\n",
        "for i, comp in enumerate(lda_model.components_):\n",
        "     vocab_comp = zip(vocab, comp)\n",
        "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:20]\n",
        "     print('\\n')\n",
        "     print(\"Topic \"+str(i)+\": \")\n",
        "     for t in sorted_words:\n",
        "            print(t[0],end=\" \")"
      ],
      "metadata": {
        "id": "vxo7P3jciVxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_results=lda_model.transform(vect_text)"
      ],
      "metadata": {
        "id": "Jq2_figSiYDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Topic'] = topic_results.argmax(axis=1)\n",
        "\n",
        "topic_map = {0:'Sports & Entertainment',\n",
        "             1:'Politics & Government',\n",
        "             2:'Politics & Economics',\n",
        "             3:'Entertainment & Pop Culture',\n",
        "             4:'Politics & Social Issues',\n",
        "             5:'Entertainment & Celebrity News',\n",
        "             6:'Environmental Issues & Natural Disasters',}\n",
        "\n",
        "df['Topic'] = df['Topic'].map(topic_map)\n",
        "\n",
        "df['Topic'].value_counts()"
      ],
      "metadata": {
        "id": "9Q4IXe2Cijdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GH7OuZAWipGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['Label_Akhir', 'Topic']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wKQJKRXenAFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}