{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYlZg0PGFPC"
      },
      "source": [
        "# **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_35fF4gMwqvv",
        "outputId": "471a9556-6743-40e1-d0f4-4f2bbffbbd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ariaghora/mpstemmer.git\n",
            "  Cloning https://github.com/ariaghora/mpstemmer.git to c:\\users\\vizza\\appdata\\local\\temp\\pip-req-build-fuf52dhk\n",
            "  Resolved https://github.com/ariaghora/mpstemmer.git to commit caedf8d9a9219e4277973b80bdbd3eecb14a7e06\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/ariaghora/mpstemmer.git 'C:\\Users\\vizza\\AppData\\Local\\Temp\\pip-req-build-fuf52dhk'\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade git+https://github.com/ariaghora/mpstemmer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX8xtdjLGDRh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p3rqPgbNnBM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5glexXlVxA"
      },
      "source": [
        "Pada bagian ini, kita akan meng-import beberapa library dasar yang sering digunakan pada pengolahan data. Library-library tersebut, yaitu Pandas dan Numpy untuk pengolahan dan manipulasi data baik dalam bentuk Array maupun DataFrame, Matplotlib dan Seaborn untuk membuat visualisasi data, dan re beserta string untuk melakukan RegEx dan pengolahan teks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrTCTrpGbL-"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAeLHMKFNUY6",
        "outputId": "dfa14cf4-d0a9-4738-9596-ea3b9400a941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Es0qZKNrhvT_O3xPJC6riqdz8fQazfSg\n",
            "To: /content/Kel5_Clickbait_Fix.xlsx\n",
            "100% 2.55M/2.55M [00:00<00:00, 175MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1Es0qZKNrhvT_O3xPJC6riqdz8fQazfSg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9P3S1tMvGvmW",
        "outputId": "bf555664-5de0-4465-c64d-e2a5f0876119"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Judul</th>\n",
              "      <th>Label_Akhir</th>\n",
              "      <th>Judul_Casefold</th>\n",
              "      <th>Judul_Relevant</th>\n",
              "      <th>Judul_Tokenized</th>\n",
              "      <th>Judul_Stemmed</th>\n",
              "      <th>Judul_Clean</th>\n",
              "      <th>Judul_Clean_Unlisted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
              "      <td>0</td>\n",
              "      <td>pemakaian masker menyebabkan penyakit legionna...</td>\n",
              "      <td>pemakaian masker menyebabkan penyakit legionna...</td>\n",
              "      <td>['pemakaian', 'masker', 'menyebabkan', 'penyak...</td>\n",
              "      <td>['pakai', 'masker', 'sebab', 'penyakit', 'legi...</td>\n",
              "      <td>['pakai', 'masker', 'penyakit', 'legionnaires']</td>\n",
              "      <td>pakai masker penyakit legionnaires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
              "      <td>0</td>\n",
              "      <td>instruksi gubernur jateng tentang penilangan  ...</td>\n",
              "      <td>instruksi gubernur jateng tentang penilangan b...</td>\n",
              "      <td>['instruksi', 'gubernur', 'jateng', 'tentang',...</td>\n",
              "      <td>['instruksi', 'gubernur', 'jateng', 'tentang',...</td>\n",
              "      <td>['instruksi', 'gubernur', 'jateng', 'tilang', ...</td>\n",
              "      <td>instruksi gubernur jateng tilang masker muka r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
              "      <td>0</td>\n",
              "      <td>foto jim rohn: jokowi adalah presiden terbaik ...</td>\n",
              "      <td>foto jim rohn jokowi adalah presiden terbaik d...</td>\n",
              "      <td>['foto', 'jim', 'rohn', 'jokowi', 'adalah', 'p...</td>\n",
              "      <td>['foto', 'jim', 'rohn', 'jokowi', 'adalah', 'p...</td>\n",
              "      <td>['foto', 'jim', 'rohn', 'jokowi', 'presiden', ...</td>\n",
              "      <td>foto jim rohn jokowi presiden dlm sejarah bang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Foto Presiden Italia menangis karena tak cukup...</td>\n",
              "      <td>0</td>\n",
              "      <td>foto presiden italia menangis karena tak cukup...</td>\n",
              "      <td>foto presiden italia menangis karena tak cukup...</td>\n",
              "      <td>['foto', 'presiden', 'italia', 'menangis', 'ka...</td>\n",
              "      <td>['foto', 'presiden', 'italia', 'tangis', 'kare...</td>\n",
              "      <td>['foto', 'presiden', 'italia', 'tangis', 'laha...</td>\n",
              "      <td>foto presiden italia tangis lahan kubur an kor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kapolres Timor Tengah Utara , Nusa Tenggara Ti...</td>\n",
              "      <td>0</td>\n",
              "      <td>kapolres timor tengah utara , nusa tenggara ti...</td>\n",
              "      <td>kapolres timor tengah utara nusa tenggara timu...</td>\n",
              "      <td>['kapolres', 'timor', 'tengah', 'utara', 'nusa...</td>\n",
              "      <td>['kapolres', 'timor', 'tengah', 'utara', 'nusa...</td>\n",
              "      <td>['kapolres', 'timor', 'utara', 'nusa', 'tengga...</td>\n",
              "      <td>kapolres timor utara nusa tenggara timur klari...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Judul  Label_Akhir  \\\n",
              "0  Pemakaian Masker Menyebabkan Penyakit Legionna...            0   \n",
              "1  Instruksi Gubernur Jateng tentang penilangan  ...            0   \n",
              "2  Foto Jim Rohn: Jokowi adalah presiden terbaik ...            0   \n",
              "3  Foto Presiden Italia menangis karena tak cukup...            0   \n",
              "4  Kapolres Timor Tengah Utara , Nusa Tenggara Ti...            0   \n",
              "\n",
              "                                      Judul_Casefold  \\\n",
              "0  pemakaian masker menyebabkan penyakit legionna...   \n",
              "1  instruksi gubernur jateng tentang penilangan  ...   \n",
              "2  foto jim rohn: jokowi adalah presiden terbaik ...   \n",
              "3  foto presiden italia menangis karena tak cukup...   \n",
              "4  kapolres timor tengah utara , nusa tenggara ti...   \n",
              "\n",
              "                                      Judul_Relevant  \\\n",
              "0  pemakaian masker menyebabkan penyakit legionna...   \n",
              "1  instruksi gubernur jateng tentang penilangan b...   \n",
              "2  foto jim rohn jokowi adalah presiden terbaik d...   \n",
              "3  foto presiden italia menangis karena tak cukup...   \n",
              "4  kapolres timor tengah utara nusa tenggara timu...   \n",
              "\n",
              "                                     Judul_Tokenized  \\\n",
              "0  ['pemakaian', 'masker', 'menyebabkan', 'penyak...   \n",
              "1  ['instruksi', 'gubernur', 'jateng', 'tentang',...   \n",
              "2  ['foto', 'jim', 'rohn', 'jokowi', 'adalah', 'p...   \n",
              "3  ['foto', 'presiden', 'italia', 'menangis', 'ka...   \n",
              "4  ['kapolres', 'timor', 'tengah', 'utara', 'nusa...   \n",
              "\n",
              "                                       Judul_Stemmed  \\\n",
              "0  ['pakai', 'masker', 'sebab', 'penyakit', 'legi...   \n",
              "1  ['instruksi', 'gubernur', 'jateng', 'tentang',...   \n",
              "2  ['foto', 'jim', 'rohn', 'jokowi', 'adalah', 'p...   \n",
              "3  ['foto', 'presiden', 'italia', 'tangis', 'kare...   \n",
              "4  ['kapolres', 'timor', 'tengah', 'utara', 'nusa...   \n",
              "\n",
              "                                         Judul_Clean  \\\n",
              "0    ['pakai', 'masker', 'penyakit', 'legionnaires']   \n",
              "1  ['instruksi', 'gubernur', 'jateng', 'tilang', ...   \n",
              "2  ['foto', 'jim', 'rohn', 'jokowi', 'presiden', ...   \n",
              "3  ['foto', 'presiden', 'italia', 'tangis', 'laha...   \n",
              "4  ['kapolres', 'timor', 'utara', 'nusa', 'tengga...   \n",
              "\n",
              "                                Judul_Clean_Unlisted  \n",
              "0                 pakai masker penyakit legionnaires  \n",
              "1  instruksi gubernur jateng tilang masker muka r...  \n",
              "2  foto jim rohn jokowi presiden dlm sejarah bang...  \n",
              "3  foto presiden italia tangis lahan kubur an kor...  \n",
              "4  kapolres timor utara nusa tenggara timur klari...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "df = pd.read_excel('C:/KULIAH/!! TSD UNAIR !!/SEMESTER 5/NATURAL LANGUAGE PROCESSING/UJIAN/Kel5_Clickbait_Fix.xlsx')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghdIZWblYd8t"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm-qDliyYfn_"
      },
      "source": [
        "## Rename & Subset Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8C50nl8THJXQ",
        "outputId": "0c1f2012-e78b-4aa3-88c9-8c27b1a3f234"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Judul_Clean_Unlisted</th>\n",
              "      <th>Label_Akhir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8802</th>\n",
              "      <td>memelanie subono kabar bj habibie tinggal jump...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9122</th>\n",
              "      <td>sembarang teknik kendara sepeda motor boks</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8252</th>\n",
              "      <td>menapol vs liverpool tekad tuan rumah kalah ju...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16046</th>\n",
              "      <td>dewan adat papua gelar aksi damai tolak rasism...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15925</th>\n",
              "      <td>nasihat kpk tsani annafari mundur</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    Judul_Clean_Unlisted  Label_Akhir\n",
              "8802   memelanie subono kabar bj habibie tinggal jump...            1\n",
              "9122          sembarang teknik kendara sepeda motor boks            1\n",
              "8252   menapol vs liverpool tekad tuan rumah kalah ju...            1\n",
              "16046  dewan adat papua gelar aksi damai tolak rasism...            0\n",
              "15925                  nasihat kpk tsani annafari mundur            0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[['Judul_Clean_Unlisted', 'Label_Akhir']]\n",
        "\n",
        "## ==== PENTING ====\n",
        "df = df[df['Label_Akhir'] != 999]\n",
        "df = df.dropna()\n",
        "df = df.sample(n=10000)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNF1-tokpGMW"
      },
      "source": [
        "# **Modelling Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7HJjXTXpP7R"
      },
      "source": [
        "## TF-IDF & N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MQJAu5LpR7M"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv_unigrams = TfidfVectorizer(ngram_range = (1,1))\n",
        "cv_bigrams = TfidfVectorizer(ngram_range = (2,2))\n",
        "cv_trigrams = TfidfVectorizer(ngram_range = (3,3))\n",
        "\n",
        "X_unigram = cv_unigrams.fit_transform(df['Judul_Clean_Unlisted']).toarray()\n",
        "y_unigram = df['Label_Akhir'].values\n",
        "\n",
        "X_bigram = cv_bigrams.fit_transform(df['Judul_Clean_Unlisted']).toarray()\n",
        "y_bigram = df['Label_Akhir'].values\n",
        "\n",
        "X_trigram = cv_trigrams.fit_transform(df['Judul_Clean_Unlisted']).toarray()\n",
        "y_trigram = df['Label_Akhir'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFdCUGctqQt6"
      },
      "source": [
        "## Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ6stEkyqRlh",
        "outputId": "6ba5baa9-917a-442d-8f52-f01ead8e4a17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    6248\n",
              "1    3752\n",
              "Name: Label_Akhir, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Label_Akhir'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENn9lF8iqZxw"
      },
      "outputs": [],
      "source": [
        "import imblearn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 42)\n",
        "X_smote_unigram, y_smote_unigram = sm.fit_resample(X_unigram, y_unigram)\n",
        "X_smote_bigram, y_smote_bigram = sm.fit_resample(X_bigram, y_bigram)\n",
        "X_smote_trigram, y_smote_trigram = sm.fit_resample(X_trigram, y_trigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUR2r6c6p8Ma"
      },
      "source": [
        "## Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RkbFsuJp6pS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "  \n",
        "# split into 70:30 ration\n",
        "X_train_uni, X_test_uni, y_train_uni, y_test_uni = train_test_split(X_smote_unigram, \n",
        "                                                                    y_smote_unigram, \n",
        "                                                                    test_size = 0.2, \n",
        "                                                                    random_state = 42)\n",
        "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X_smote_bigram, \n",
        "                                                                y_smote_bigram, \n",
        "                                                                test_size = 0.2, \n",
        "                                                                random_state = 42)\n",
        "X_train_tri, X_test_tri, y_train_tri, y_test_tri = train_test_split(X_smote_trigram, \n",
        "                                                                    y_smote_trigram, \n",
        "                                                                    test_size = 0.2, \n",
        "                                                                    random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSobZ3xWrp2A"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQWZhzZsrroH"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ7DeMimro5q"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_classifier_uni = MultinomialNB()\n",
        "nb_classifier_bi = MultinomialNB()\n",
        "nb_classifier_tri = MultinomialNB()\n",
        "nb_classifier_uni.fit(X_train_uni, y_train_uni.ravel())\n",
        "nb_classifier_bi.fit(X_train_bi, y_train_bi.ravel())\n",
        "nb_classifier_tri.fit(X_train_tri, y_train_tri.ravel())\n",
        "\n",
        "y_pred_nb_uni = nb_classifier_uni.predict(X_test_uni)\n",
        "y_pred_nb_bi = nb_classifier_bi.predict(X_test_bi)\n",
        "y_pred_nb_tri = nb_classifier_tri.predict(X_test_tri)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_nb_uni = round(accuracy_score(y_test_uni, y_pred_nb_uni),2)\n",
        "precision_nb_uni = round(precision_score(y_test_uni, y_pred_nb_uni, average='weighted'),2)\n",
        "recall_nb_uni = round(recall_score(y_test_uni, y_pred_nb_uni, average='weighted'),2)\n",
        "f1score_nb_uni = round(f1_score(y_test_uni, y_pred_nb_uni, average='weighted'),2)\n",
        "\n",
        "akurasi_nb_bi = round(accuracy_score(y_test_bi, y_pred_nb_bi),2)\n",
        "precision_nb_bi = round(precision_score(y_test_bi, y_pred_nb_bi, average='weighted'),2)\n",
        "recall_nb_bi = round(recall_score(y_test_bi, y_pred_nb_bi, average='weighted'),2)\n",
        "f1score_nb_bi = round(f1_score(y_test_bi, y_pred_nb_bi, average='weighted'),2)\n",
        "\n",
        "akurasi_nb_tri = round(accuracy_score(y_test_tri, y_pred_nb_tri),2)\n",
        "precision_nb_tri = round(precision_score(y_test_tri, y_pred_nb_tri, average='weighted'),2)\n",
        "recall_nb_tri = round(recall_score(y_test_tri, y_pred_nb_tri, average='weighted'),2)\n",
        "f1score_nb_tri = round(f1_score(y_test_tri, y_pred_nb_tri, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnG3YsNhtII1"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT9iRD30tJOy"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_classifier_uni = RandomForestClassifier(random_state = 42)\n",
        "rf_classifier_bi = RandomForestClassifier(random_state = 42)\n",
        "rf_classifier_tri = RandomForestClassifier(random_state = 42)\n",
        "rf_classifier_uni.fit(X_train_uni, y_train_uni.ravel())\n",
        "rf_classifier_bi.fit(X_train_bi, y_train_bi.ravel())\n",
        "rf_classifier_tri.fit(X_train_tri, y_train_tri.ravel())\n",
        "\n",
        "y_pred_rf_uni = rf_classifier_uni.predict(X_test_uni)\n",
        "y_pred_rf_bi = rf_classifier_bi.predict(X_test_bi)\n",
        "y_pred_rf_tri = rf_classifier_tri.predict(X_test_tri)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_rf_uni = round(accuracy_score(y_test_uni, y_pred_rf_uni),2)\n",
        "precision_rf_uni = round(precision_score(y_test_uni, y_pred_rf_uni, average='weighted'),2)\n",
        "recall_rf_uni = round(recall_score(y_test_uni, y_pred_rf_uni, average='weighted'),2)\n",
        "f1score_rf_uni = round(f1_score(y_test_uni, y_pred_rf_uni, average='weighted'),2)\n",
        "\n",
        "akurasi_rf_bi = round(accuracy_score(y_test_bi, y_pred_rf_bi),2)\n",
        "precision_rf_bi = round(precision_score(y_test_bi, y_pred_rf_bi, average='weighted'),2)\n",
        "recall_rf_bi = round(recall_score(y_test_bi, y_pred_rf_bi, average='weighted'),2)\n",
        "f1score_rf_bi = round(f1_score(y_test_bi, y_pred_rf_bi, average='weighted'),2)\n",
        "\n",
        "akurasi_rf_tri = round(accuracy_score(y_test_tri, y_pred_rf_tri),2)\n",
        "precision_rf_tri = round(precision_score(y_test_tri, y_pred_rf_tri, average='weighted'),2)\n",
        "recall_rf_tri = round(recall_score(y_test_tri, y_pred_rf_tri, average='weighted'),2)\n",
        "f1score_rf_tri = round(f1_score(y_test_tri, y_pred_rf_tri, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n5QqrRHtv8P"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec4A9Pt3txqJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_classifier_uni = SVC(kernel = 'linear')\n",
        "svm_classifier_bi = SVC(kernel = 'linear')\n",
        "svm_classifier_tri = SVC(kernel = 'linear')\n",
        "svm_classifier_uni.fit(X_train_uni, y_train_uni.ravel())\n",
        "svm_classifier_bi.fit(X_train_bi, y_train_bi.ravel())\n",
        "svm_classifier_tri.fit(X_train_tri, y_train_tri.ravel())\n",
        "\n",
        "y_pred_svm_uni = svm_classifier_uni.predict(X_test_uni)\n",
        "y_pred_svm_bi = svm_classifier_bi.predict(X_test_bi)\n",
        "y_pred_svm_tri = svm_classifier_tri.predict(X_test_tri)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_svm_uni = round(accuracy_score(y_test_uni, y_pred_svm_uni),2)\n",
        "precision_svm_uni = round(precision_score(y_test_uni, y_pred_svm_uni, average='weighted'),2)\n",
        "recall_svm_uni = round(recall_score(y_test_uni, y_pred_svm_uni, average='weighted'),2)\n",
        "f1score_svm_uni = round(f1_score(y_test_uni, y_pred_svm_uni, average='weighted'),2)\n",
        "\n",
        "akurasi_svm_bi = round(accuracy_score(y_test_bi, y_pred_svm_bi),2)\n",
        "precision_svm_bi = round(precision_score(y_test_bi, y_pred_svm_bi, average='weighted'),2)\n",
        "recall_svm_bi = round(recall_score(y_test_bi, y_pred_svm_bi, average='weighted'),2)\n",
        "f1score_svm_bi = round(f1_score(y_test_bi, y_pred_svm_bi, average='weighted'),2)\n",
        "\n",
        "akurasi_svm_tri = round(accuracy_score(y_test_tri, y_pred_svm_tri),2)\n",
        "precision_svm_tri = round(precision_score(y_test_tri, y_pred_svm_tri, average='weighted'),2)\n",
        "recall_svm_tri = round(recall_score(y_test_tri, y_pred_svm_tri, average='weighted'),2)\n",
        "f1score_svm_tri = round(f1_score(y_test_tri, y_pred_svm_tri, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqc8NmSiueVa"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7hiXRV1Gpdt",
        "outputId": "755857e0-2ada-49d0-92e5-885083b4c5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightgbm\n",
            "  Using cached lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\python310\\lib\\site-packages (from lightgbm) (1.1.3)\n",
            "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from lightgbm) (1.23.4)\n",
            "Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (from lightgbm) (1.9.3)\n",
            "Requirement already satisfied: wheel in c:\\python310\\lib\\site-packages (from lightgbm) (0.38.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
            "Installing collected packages: lightgbm\n",
            "Successfully installed lightgbm-3.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAos9DsXubuo",
        "outputId": "798e6b94-2cfb-47f5-f0d4-0895c5119f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "lgb_classifier_uni = lgb.LGBMClassifier(random_state=42, boosting = 'gbdt', n_estimators=1000,\n",
        "                                        tree_learner = 'voting', deterministic=True)\n",
        "lgb_classifier_bi = lgb.LGBMClassifier(random_state=42, boosting = 'gbdt', n_estimators=1000,\n",
        "                                       tree_learner = 'voting', deterministic=True)\n",
        "lgb_classifier_tri = lgb.LGBMClassifier(random_state=42, boosting = 'gbdt', n_estimators=1000, \n",
        "                                        tree_learner = 'voting', deterministic=True)\n",
        "lgb_classifier_uni.fit(X_train_uni, y_train_uni.ravel())\n",
        "lgb_classifier_bi.fit(X_train_bi, y_train_bi.ravel())\n",
        "lgb_classifier_tri.fit(X_train_tri, y_train_tri.ravel())\n",
        "\n",
        "y_pred_lgb_uni = lgb_classifier_uni.predict(X_test_uni)\n",
        "y_pred_lgb_bi = lgb_classifier_bi.predict(X_test_bi)\n",
        "y_pred_lgb_tri = lgb_classifier_tri.predict(X_test_tri)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_lgb_uni = round(accuracy_score(y_test_uni, y_pred_lgb_uni),2)\n",
        "precision_lgb_uni = round(precision_score(y_test_uni, y_pred_lgb_uni, average='weighted'),2)\n",
        "recall_lgb_uni = round(recall_score(y_test_uni, y_pred_lgb_uni, average='weighted'),2)\n",
        "f1score_lgb_uni = round(f1_score(y_test_uni, y_pred_lgb_uni, average='weighted'),2)\n",
        "\n",
        "akurasi_lgb_bi = round(accuracy_score(y_test_bi, y_pred_lgb_bi),2)\n",
        "precision_lgb_bi = round(precision_score(y_test_bi, y_pred_lgb_bi, average='weighted'),2)\n",
        "recall_lgb_bi = round(recall_score(y_test_bi, y_pred_lgb_bi, average='weighted'),2)\n",
        "f1score_lgb_bi = round(f1_score(y_test_bi, y_pred_lgb_bi, average='weighted'),2)\n",
        "\n",
        "akurasi_lgb_tri = round(accuracy_score(y_test_tri, y_pred_lgb_tri),2)\n",
        "precision_lgb_tri = round(precision_score(y_test_tri, y_pred_lgb_tri, average='weighted'),2)\n",
        "recall_lgb_tri = round(recall_score(y_test_tri, y_pred_lgb_tri, average='weighted'),2)\n",
        "f1score_lgb_tri = round(f1_score(y_test_tri, y_pred_lgb_tri, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Keyqip9vVLb"
      },
      "source": [
        "# **Model Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2PdSY5xvYWj",
        "outputId": "55324b5d-622f-4097-bc53-5c02db0eb002"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>N-grams</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>Unigrams</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>Bigrams</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>Trigrams</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Unigrams</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Bigrams</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Trigrams</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>Unigrams</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>Bigrams</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>Trigrams</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>Unigrams</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>Bigrams</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>Trigrams</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Model   N-grams  Accuracy  Precision  Recall  F1-Score\n",
              "0             Naive Bayes  Unigrams      0.75       0.75    0.75      0.75\n",
              "1             Naive Bayes   Bigrams      0.70       0.73    0.70      0.69\n",
              "2             Naive Bayes  Trigrams      0.55       0.62    0.55      0.47\n",
              "0           Random Forest  Unigrams      0.79       0.80    0.79      0.79\n",
              "1           Random Forest   Bigrams      0.74       0.78    0.74      0.73\n",
              "2           Random Forest  Trigrams      0.72       0.78    0.72      0.71\n",
              "0  Support Vector Machine  Unigrams      0.77       0.77    0.77      0.77\n",
              "1  Support Vector Machine   Bigrams      0.77       0.78    0.77      0.77\n",
              "2  Support Vector Machine  Trigrams      0.75       0.79    0.75      0.74\n",
              "0                LightGBM  Unigrams      0.74       0.74    0.74      0.74\n",
              "1                LightGBM   Bigrams      0.59       0.67    0.59      0.54\n",
              "2                LightGBM  Trigrams      0.50       0.59    0.50      0.35"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_nb = pd.DataFrame({'Model':['Naive Bayes', 'Naive Bayes', 'Naive Bayes'],\n",
        "                        'N-grams':['Unigrams', 'Bigrams', 'Trigrams'],\n",
        "                        'Accuracy':[akurasi_nb_uni, akurasi_nb_bi, akurasi_nb_tri],\n",
        "                        'Precision':[precision_nb_uni, precision_nb_bi, precision_nb_tri],\n",
        "                        'Recall':[recall_nb_uni, recall_nb_bi, recall_nb_tri],\n",
        "                        'F1-Score':[f1score_nb_uni, f1score_nb_bi, f1score_nb_tri]})\n",
        "\n",
        "data_rf = pd.DataFrame({'Model':['Random Forest', 'Random Forest', 'Random Forest'],\n",
        "                        'N-grams':['Unigrams', 'Bigrams', 'Trigrams'],\n",
        "                        'Accuracy':[akurasi_rf_uni, akurasi_rf_bi, akurasi_rf_tri],\n",
        "                        'Precision':[precision_rf_uni, precision_rf_bi, precision_rf_tri],\n",
        "                        'Recall':[recall_rf_uni, recall_rf_bi, recall_rf_tri],\n",
        "                        'F1-Score':[f1score_rf_uni, f1score_rf_bi, f1score_rf_tri]})\n",
        "\n",
        "data_svm = pd.DataFrame({'Model':['Support Vector Machine', 'Support Vector Machine', 'Support Vector Machine'],\n",
        "                        'N-grams':['Unigrams', 'Bigrams', 'Trigrams'],\n",
        "                        'Accuracy':[akurasi_svm_uni, akurasi_svm_bi, akurasi_svm_tri],\n",
        "                        'Precision':[precision_svm_uni, precision_svm_bi, precision_svm_tri],\n",
        "                        'Recall':[recall_svm_uni, recall_svm_bi, recall_svm_tri],\n",
        "                        'F1-Score':[f1score_svm_uni, f1score_svm_bi, f1score_svm_tri]})\n",
        "\n",
        "data_lgbm = pd.DataFrame({'Model':['LightGBM', 'LightGBM', 'LightGBM'],\n",
        "                        'N-grams':['Unigrams', 'Bigrams', 'Trigrams'],\n",
        "                        'Accuracy':[akurasi_lgb_uni, akurasi_lgb_bi, akurasi_lgb_tri],\n",
        "                        'Precision':[precision_lgb_uni, precision_lgb_bi, precision_lgb_tri],\n",
        "                        'Recall':[recall_lgb_uni, recall_lgb_bi, recall_lgb_tri],\n",
        "                        'F1-Score':[f1score_lgb_uni, f1score_lgb_bi, f1score_lgb_tri]})\n",
        "\n",
        "data_performa = data_nb.append(data_rf).append(data_svm).append(data_lgbm)\n",
        "data_performa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPnwKztKD3M7"
      },
      "outputs": [],
      "source": [
        "data_performa.to_excel('Data_Klasifikasi_Clickbait.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMH51WOdXB3B"
      },
      "source": [
        "# **Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eVJIg3kXHHJ"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "file_model = 'Model_Final_Clickbait.sav'\n",
        "pkl.dump(rf_classifier_uni, open(file_model, 'wb'))\n",
        "\n",
        "file_tfidf = 'TFIDF_Final_Clickbait.pickle'\n",
        "pkl.dump(cv_unigrams, open(file_tfidf, 'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}